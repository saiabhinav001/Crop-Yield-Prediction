{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cb16774",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "544cd4fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to process weather files...\n",
      "  ✅ Successfully processed anugul.csv for ANUGUL\n",
      "  ✅ Successfully processed balangir.csv for BALANGIR\n",
      "  ✅ Successfully processed baleshwar.csv for BALESHWAR\n",
      "  ✅ Successfully processed bargarh.csv for BARGARH\n",
      "  ✅ Successfully processed bhadrak.csv for BHADRAK\n",
      "  ✅ Successfully processed boudh.csv for BOUDH\n",
      "  ✅ Successfully processed cuttack.csv for CUTTACK\n",
      "  ✅ Successfully processed deogarh.csv for DEOGARH\n",
      "  ✅ Successfully processed dhenkanal.csv for DHENKANAL\n",
      "  ✅ Successfully processed gajapati.csv for GAJAPATI\n",
      "  ✅ Successfully processed ganjam.csv for GANJAM\n",
      "  ✅ Successfully processed jagatsinghpur.csv for JAGATSINGHPUR\n",
      "  ✅ Successfully processed jajapur.csv for JAJAPUR\n",
      "  ✅ Successfully processed jharsuguda.csv for JHARSUGUDA\n",
      "  ✅ Successfully processed kalahandi.csv for KALAHANDI\n",
      "  ✅ Successfully processed kandhamal.csv for KANDHAMAL\n",
      "  ✅ Successfully processed kendrapara.csv for KENDRAPARA\n",
      "  ✅ Successfully processed kendujhar.csv for KENDUJHAR\n",
      "  ✅ Successfully processed khordha.csv for KHORDHA\n",
      "  ✅ Successfully processed koraput.csv for KORAPUT\n",
      "  ✅ Successfully processed malkangiri.csv for MALKANGIRI\n",
      "  ✅ Successfully processed mayurbhanj.csv for MAYURBHANJ\n",
      "  ✅ Successfully processed nabarangpur.csv for NABARANGPUR\n",
      "  ✅ Successfully processed nayagarh.csv for NAYAGARH\n",
      "  ✅ Successfully processed nuapada.csv for NUAPADA\n",
      "  ✅ Successfully processed puri.csv for PURI\n",
      "  ✅ Successfully processed rayagada.csv for RAYAGADA\n",
      "  ✅ Successfully processed sambalpur.csv for SAMBALPUR\n",
      "  ✅ Successfully processed sonepur.csv for SONEPUR\n",
      "  ✅ Successfully processed sundargarh.csv for SUNDARGARH\n",
      "\n",
      "✅ All weather files have been processed and combined!\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# PART 1: PROCESS ALL 30 DISTRICT WEATHER FILES\n",
    "# ==============================================================================\n",
    "district_filenames = {\n",
    "    'ANUGUL':'anugul.csv',\n",
    "    'BALANGIR':'balangir.csv',\n",
    "    'BALESHWAR':'baleshwar.csv',\n",
    "    'BARGARH':'bargarh.csv',\n",
    "    'BHADRAK':'bhadrak.csv',\n",
    "    'BOUDH':'boudh.csv',\n",
    "    'CUTTACK':'cuttack.csv',\n",
    "    'DEOGARH':'deogarh.csv',\n",
    "    'DHENKANAL':'dhenkanal.csv',\n",
    "    'GAJAPATI':'gajapati.csv',\n",
    "    'GANJAM':'ganjam.csv',\n",
    "    'JAGATSINGHPUR':'jagatsinghpur.csv',\n",
    "    'JAJAPUR':'jajapur.csv',\n",
    "    'JHARSUGUDA':'jharsuguda.csv',\n",
    "    'KALAHANDI':'kalahandi.csv',\n",
    "    'KANDHAMAL':'kandhamal.csv',\n",
    "    'KENDRAPARA':'kendrapara.csv',\n",
    "    'KENDUJHAR':'kendujhar.csv',\n",
    "    'KHORDHA':'khordha.csv',\n",
    "    'KORAPUT':'koraput.csv',\n",
    "    'MALKANGIRI':'malkangiri.csv',\n",
    "    'MAYURBHANJ':'mayurbhanj.csv',\n",
    "    'NABARANGPUR':'nabarangpur.csv',\n",
    "    'NAYAGARH':'nayagarh.csv',\n",
    "    'NUAPADA':'nuapada.csv',\n",
    "    'PURI': 'puri.csv',\n",
    "    'RAYAGADA':'rayagada.csv',\n",
    "    'SAMBALPUR':'sambalpur.csv',\n",
    "    'SONEPUR':'sonepur.csv',\n",
    "    'SUNDARGARH':'sundargarh.csv'\n",
    "}\n",
    "\n",
    "# An empty list to store the processed data for each district\n",
    "all_weather_data = []\n",
    "\n",
    "def get_season(month):\n",
    "    if 6 <= month <= 10: return 'Kharif'\n",
    "    elif month in [11, 12, 1, 2, 3]: return 'Rabi'\n",
    "    else: return 'Summer'\n",
    "\n",
    "print(\"Starting to process weather files...\")\n",
    "for district_name, file_name in district_filenames.items():\n",
    "    try:\n",
    "        temp_df = pd.read_csv(file_name, skiprows=13)\n",
    "\n",
    "        # Create a proper date from YEAR and Day of Year (DOY)\n",
    "        temp_df['DATE'] = pd.to_datetime(temp_df['YEAR'].astype(str) + temp_df['DOY'].astype(str), format='%Y%j')\n",
    "        # Extract the month from our new DATE column\n",
    "        temp_df['Month'] = temp_df['DATE'].dt.month\n",
    "\n",
    "        temp_df['Season'] = temp_df['Month'].apply(get_season)\n",
    "        \n",
    "        seasonal_data = temp_df.groupby(['YEAR', 'Season']).agg(\n",
    "            avg_temp=('T2M', 'mean'),\n",
    "            max_temp=('T2M_MAX', 'max'),\n",
    "            min_temp=('T2M_MIN', 'min'),\n",
    "            total_rainfall=('PRECTOTCORR', 'sum')\n",
    "        ).reset_index()\n",
    "        \n",
    "        seasonal_data.rename(columns={'YEAR': 'Year'}, inplace=True)\n",
    "        seasonal_data['District_Name'] = district_name\n",
    "        all_weather_data.append(seasonal_data)\n",
    "        print(f\"  ✅ Successfully processed {file_name} for {district_name}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  ❌ ERROR: File not found for {district_name}: '{file_name}'. Please check the filename. Skipping.\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ ERROR: Could not process {file_name}. Reason: {e}\")\n",
    "\n",
    "final_weather_df = pd.concat(all_weather_data, ignore_index=True)\n",
    "print(\"\\n✅ All weather files have been processed and combined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b6314a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading and cleaning the main crop production data (APY.csv)...\n",
      "✅ Crop data loaded and cleaned.\n",
      "\n",
      "Merging crop data with weather data...\n",
      "✅ Merge complete! Your master dataset is ready.\n",
      "\n",
      "--- MASTER DATASET READY FOR MODEL TRAINING ---\n",
      "Empty DataFrame\n",
      "Columns: [State, District_Name, Crop, Year, Season, Area, Production, Yield, avg_temp, max_temp, min_temp, total_rainfall]\n",
      "Index: []\n",
      "\n",
      "Your final dataset has 0 rows.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# PART 2 & 3: LOAD CROP DATA AND MERGE\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\nLoading and cleaning the main crop production data (APY.csv)...\")\n",
    "df_crop = pd.read_csv('APY.csv')\n",
    "df_crop.columns = df_crop.columns.str.strip()\n",
    "df_odisha = df_crop[df_crop['State'] == 'Odisha'].copy()\n",
    "df_odisha.rename(columns={'Crop_Year': 'Year', 'District': 'District_Name'}, inplace=True)\n",
    "print(\"✅ Crop data loaded and cleaned.\")\n",
    "\n",
    "print(\"\\nMerging crop data with weather data...\")\n",
    "master_df = pd.merge(df_odisha, final_weather_df, on=['District_Name', 'Year', 'Season'], how='left')\n",
    "master_df.dropna(subset=['avg_temp'], inplace=True)\n",
    "print(\"✅ Merge complete! Your master dataset is ready.\")\n",
    "\n",
    "print(\"\\n--- MASTER DATASET READY FOR MODEL TRAINING ---\")\n",
    "print(master_df.head())\n",
    "print(f\"\\nYour final dataset has {len(master_df)} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b247e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1dcf488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to process weather files...\n",
      "\n",
      "✅ All weather files have been processed and combined!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 1: PROCESS ALL 30 DISTRICT WEATHER FILES\n",
    "# ==============================================================================\n",
    "\n",
    "# Using the dictionary you provided\n",
    "district_filenames = {\n",
    "    'ANUGUL':'anugul.csv',\n",
    "    'BALANGIR':'balangir.csv',\n",
    "    'BALESHWAR':'baleshwar.csv',\n",
    "    'BARGARH':'bargarh.csv',\n",
    "    'BHADRAK':'bhadrak.csv',\n",
    "    'BOUDH':'boudh.csv',\n",
    "    'CUTTACK':'cuttack.csv',\n",
    "    'DEOGARH':'deogarh.csv',\n",
    "    'DHENKANAL':'dhenkanal.csv',\n",
    "    'GAJAPATI':'gajapati.csv',\n",
    "    'GANJAM':'ganjam.csv',\n",
    "    'JAGATSINGHPUR':'jagatsinghpur.csv',\n",
    "    'JAJAPUR':'jajapur.csv',\n",
    "    'JHARSUGUDA':'jharsuguda.csv',\n",
    "    'KALAHANDI':'kalahandi.csv',\n",
    "    'KANDHAMAL':'kandhamal.csv',\n",
    "    'KENDRAPARA':'kendrapara.csv',\n",
    "    'KENDUJHAR':'kendujhar.csv',\n",
    "    'KHORDHA':'khordha.csv',\n",
    "    'KORAPUT':'koraput.csv',\n",
    "    'MALKANGIRI':'malkangiri.csv',\n",
    "    'MAYURBHANJ':'mayurbhanj.csv',\n",
    "    'NABARANGPUR':'nabarangpur.csv',\n",
    "    'NAYAGARH':'nayagarh.csv',\n",
    "    'NUAPADA':'nuapada.csv',\n",
    "    'PURI': 'puri.csv',\n",
    "    'RAYAGADA':'rayagada.csv',\n",
    "    'SAMBALPUR':'sambalpur.csv',\n",
    "    'SONEPUR':'sonepur.csv',\n",
    "    'SUNDARGARH':'sundargarh.csv'\n",
    "}\n",
    "\n",
    "all_weather_data = []\n",
    "\n",
    "def get_season(month):\n",
    "    if 6 <= month <= 10: return 'Kharif'\n",
    "    elif month in [11, 12, 1, 2, 3]: return 'Rabi'\n",
    "    else: return 'Summer'\n",
    "\n",
    "print(\"Starting to process weather files...\")\n",
    "for district_name, file_name in district_filenames.items():\n",
    "    try:\n",
    "        temp_df = pd.read_csv(file_name, skiprows=13)\n",
    "        temp_df['DATE'] = pd.to_datetime(temp_df['YEAR'].astype(str) + temp_df['DOY'].astype(str), format='%Y%j')\n",
    "        temp_df['Month'] = temp_df['DATE'].dt.month\n",
    "        temp_df['Season'] = temp_df['Month'].apply(get_season)\n",
    "        seasonal_data = temp_df.groupby(['YEAR', 'Season']).agg(avg_temp=('T2M', 'mean'),max_temp=('T2M_MAX', 'max'),min_temp=('T2M_MIN', 'min'),total_rainfall=('PRECTOTCORR', 'sum')).reset_index()\n",
    "        seasonal_data.rename(columns={'YEAR': 'Year'}, inplace=True)\n",
    "        seasonal_data['District_Name'] = district_name\n",
    "        all_weather_data.append(seasonal_data)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  ❌ ERROR: File not found for {district_name}: '{file_name}'.\")\n",
    "final_weather_df = pd.concat(all_weather_data, ignore_index=True)\n",
    "print(\"\\n✅ All weather files have been processed and combined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61dd8b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading and cleaning the main crop production data (APY.csv)...\n",
      "✅ Crop data loaded.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# PART 2: LOAD AND PREPARE CROP DATA\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\nLoading and cleaning the main crop production data (APY.csv)...\")\n",
    "df_crop = pd.read_csv('APY.csv')\n",
    "df_crop.columns = df_crop.columns.str.strip()\n",
    "df_odisha = df_crop[df_crop['State'] == 'Odisha'].copy()\n",
    "df_odisha.rename(columns={'Crop_Year': 'Year', 'District': 'District_Name'}, inplace=True)\n",
    "print(\"✅ Crop data loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52528812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- DEBUGGING: CHECKING KEYS BEFORE MERGE ---\n",
      "\n",
      "1. Unique districts in Crop Data:\n",
      "['ANUGUL', 'BALANGIR', 'BALESHWAR', 'BARGARH', 'BHADRAK', 'BOUDH', 'CUTTACK', 'DEOGARH', 'DHENKANAL', 'GAJAPATI', 'GANJAM', 'JAGATSINGHAPUR', 'JAJAPUR', 'JHARSUGUDA', 'KALAHANDI', 'KANDHAMAL', 'KENDRAPARA', 'KENDUJHAR', 'KHORDHA', 'KORAPUT', 'MALKANGIRI', 'MAYURBHANJ', 'NABARANGPUR', 'NAYAGARH', 'NUAPADA', 'PURI', 'RAYAGADA', 'SAMBALPUR', 'SONEPUR', 'SUNDARGARH']\n",
      "\n",
      "2. Unique districts in Weather Data:\n",
      "['ANUGUL', 'BALANGIR', 'BALESHWAR', 'BARGARH', 'BHADRAK', 'BOUDH', 'CUTTACK', 'DEOGARH', 'DHENKANAL', 'GAJAPATI', 'GANJAM', 'JAGATSINGHPUR', 'JAJAPUR', 'JHARSUGUDA', 'KALAHANDI', 'KANDHAMAL', 'KENDRAPARA', 'KENDUJHAR', 'KHORDHA', 'KORAPUT', 'MALKANGIRI', 'MAYURBHANJ', 'NABARANGPUR', 'NAYAGARH', 'NUAPADA', 'PURI', 'RAYAGADA', 'SAMBALPUR', 'SONEPUR', 'SUNDARGARH']\n",
      "\n",
      "3. Unique seasons in Crop Data (after cleaning spaces):\n",
      "['Autumn' 'Summer' 'Winter' 'Kharif' 'Rabi' 'Whole Year']\n",
      "\n",
      "4. Unique seasons in Weather Data:\n",
      "['Kharif' 'Rabi' 'Summer']\n",
      "\n",
      "--- END DEBUGGING ---\n",
      "\n",
      "✅ Merge complete!\n",
      "\n",
      "--- MASTER DATASET READY FOR MODEL TRAINING ---\n",
      "    State District_Name_x       Crop  Year     Season_x    Area  Production  \\\n",
      "1  Odisha          ANUGUL  Arhar/Tur  1997  Summer        469.0       115.0   \n",
      "3  Odisha          ANUGUL  Arhar/Tur  1999  Kharif       7960.0      5010.0   \n",
      "4  Odisha          ANUGUL  Arhar/Tur  2000  Kharif       8930.0      6430.0   \n",
      "5  Odisha          ANUGUL  Arhar/Tur  2002  Kharif       8730.0      6050.0   \n",
      "6  Odisha          ANUGUL  Arhar/Tur  2003  Kharif       9500.0      6500.0   \n",
      "\n",
      "   Yield Season_y   avg_temp  max_temp  min_temp  total_rainfall  \\\n",
      "1   0.25   Summer  31.696393     45.56     18.99          135.95   \n",
      "3   0.63   Kharif  26.761830     41.06     16.73         1217.58   \n",
      "4   0.72   Kharif  26.927124     37.95     15.42          856.98   \n",
      "5   0.69   Kharif  27.205163     41.70     15.52         1014.24   \n",
      "6   0.68   Kharif  27.460915     46.23     18.55         1639.26   \n",
      "\n",
      "  District_Name_y  \n",
      "1          ANUGUL  \n",
      "3          ANUGUL  \n",
      "4          ANUGUL  \n",
      "5          ANUGUL  \n",
      "6          ANUGUL  \n",
      "\n",
      "Your final dataset has 7227 rows.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# PART 3: DEBUG AND MERGE\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n--- DEBUGGING: CHECKING KEYS BEFORE MERGE ---\")\n",
    "\n",
    "# Standardize the keys\n",
    "df_odisha['Season_Clean'] = df_odisha['Season'].str.strip()\n",
    "df_odisha['District_Name_Clean'] = df_odisha['District_Name'].str.upper()\n",
    "\n",
    "# Print the unique values from both DataFrames\n",
    "print(\"\\n1. Unique districts in Crop Data:\")\n",
    "print(sorted(df_odisha['District_Name_Clean'].unique()))\n",
    "\n",
    "print(\"\\n2. Unique districts in Weather Data:\")\n",
    "print(sorted(final_weather_df['District_Name'].unique()))\n",
    "\n",
    "print(\"\\n3. Unique seasons in Crop Data (after cleaning spaces):\")\n",
    "print(df_odisha['Season_Clean'].unique())\n",
    "\n",
    "print(\"\\n4. Unique seasons in Weather Data:\")\n",
    "print(final_weather_df['Season'].unique())\n",
    "\n",
    "print(\"\\n--- END DEBUGGING ---\")\n",
    "\n",
    "\n",
    "# Perform the merge using the CLEANED columns\n",
    "master_df = pd.merge(df_odisha, final_weather_df,\n",
    "                     left_on=['District_Name_Clean', 'Year', 'Season_Clean'],\n",
    "                     right_on=['District_Name', 'Year', 'Season'],\n",
    "                     how='left')\n",
    "\n",
    "# Drop helper columns\n",
    "master_df = master_df.drop(columns=['District_Name_Clean', 'Season_Clean'])\n",
    "\n",
    "master_df.dropna(subset=['avg_temp'], inplace=True)\n",
    "print(\"\\n✅ Merge complete!\")\n",
    "print(\"\\n--- MASTER DATASET READY FOR MODEL TRAINING ---\")\n",
    "print(master_df.head())\n",
    "print(f\"\\nYour final dataset has {len(master_df)} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7043f25b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
